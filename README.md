<h1 align="center">üå± Treinamento de Modelos üåç</h1>

---

## üìñ O que o script faz?

O Fluxo do script √© separado em partes entre modelos, orquestrador e notebooks. Foram escolhidos 3 modelos preditivos **com base nos conhecimentos estat√≠sticos do time de dados do segundo ano**, criamos um arquivo `.py` para cada um dos modelos e tamb√©m o arquivo `choose_a_model.py`, que executa e compara cada modelo, para assim, salvar somente o modelo com melhor desempenho na pasta `best_model`. Os notebooks servem como refer√™ncia de fluxo em cada modelo, al√©m de serem a primeira vers√£o de cada um. O target de cada modelo √© **`nivel_emissao`**, a fim de validar a pegada de carbono do usu√°rio baseado nos dados fornecidos.

> üí° **Nota:** Refor√ßamos que os notebooks presentes neste diret√≥rio servem **√∫nica** e **exclusivamente** como apoio visual. O funcionamento correto de cada modelo est√° presente em cada arquivo `.py` com o mesmo nome do modelo utilizado

---

## üé≤ Sobre os dados utilizados

Os dados v√™m de duas fontes diferentes: `PostgresSQL` e `MongoDB`. 

## üè¶ PostgresSQL

| Coluna | Descri√ß√£o |
|:-------|:-----------|
| **`numero_cracha`** | ID do funcion√°rio, vem da tabela funcion√°rio |
| **`nivel_cargo`** | Cargo alto, m√©dio ou baixo √© o `nivel_cargo`, vem da tabela cargo|
| **`cidade`** | Cidade de resid√™ncia do funcion√°rio, vem da tabela localiza√ß√£o |
| **`estado`** | Estado de resid√™ncia do funcion√°rio, vem da tabela localiza√ß√£o |
| **`categoria`** | Tipo de categoria da empresa que o funcion√°rio trabalha (Aliment√≠cio, Energia etc), vem da tabela categoria_empresa |


## üè¶ MongoDB

| Coluna | Descri√ß√£o |
|:-------|:-----------|
| **`numero_cracha`** | ID do funcion√°rio, vem da collection formulario para relacionar com o numero_cracha do sql |
| **`nivel_emissao`** | Emiss√£o alta, m√©dia ou baixa, vem da collection formulario |


> üí°**Nota** A divis√£o de treino e teste foram separados em *80%* de treino e *20%* de teste, como padr√£o de divis√£o. Cada banco tem no m√≠nimo 10k de dados.

---

## ‚úÖ Modelos escolhidos e par√¢metros utilizados


Cada modelo foi feito usando pipelines fornecidas pela biblioteca do `scikit-learn`, cada um deles recebe um **preprocessador** com a seguinte estrutura:
- Recebe as colunas num√©ricas e categ√≥ricas
- Valida a exist√™ncia para cada tipo de coluna (pode existir somente colunas categ√≥ricas etc)
- Aplica a padroniza√ß√£o necess√°rio para cada tipo de dado
#### Por que escolhemos cada padroniza√ß√£o? 
| Tipo | Descri√ß√£o |
|:-------|:-----------|
| **`SimpleImputer(strategy="mean")`** | O `SimpleImputer` substitui valores ausentes pela m√©dia da coluna, a m√©dia mant√©m a distribui√ß√£o dos dados e evita enviesar o modelo com substitui√ß√µes arbitr√°rias. |
| **`MinMaxScaler()`** | O `MinMaxScaler()` transforma todos os valores num√©ricos para o intervalo `[0, 1]`, Diferente do StandardScaler, o MinMaxScaler preserva a forma original da distribui√ß√£o e √© mais vers√°til para dados com limites conhecidos ou que ser√£o usados em algoritmos baseados em dist√¢ncia. Tamb√©m √© √∫til para modelos sens√≠veis a magnitude (como o KNN) |
| **`SimpleImputer(strategy="most_frequent")`** | O `SimpleImputer(strategy="most_frequent")` substitui valores ausentes pela categoria mais frequente, evita perda de dados e mant√©m coer√™ncia sem criar novas classes artificiais. Essa abordagem funciona bem em qualquer tipo de modelo  |
| **`OneHotEncoder(handle_unknown="ignore")`** | O `OneHotEncoder(handle_unknown="ignore")` converte vari√°veis categ√≥ricas em vari√°veis bin√°rias (dummies), cria uma representa√ß√£o num√©rica compat√≠vel com qualquer modelo de ML. O par√¢metro `handle_unknown="ignore"` evita erros quando aparecem categorias in√©ditas no conjunto de teste, garantindo generaliza√ß√£o segura. |

O `ColumnTransformer` aplica os pipelines adequados (num√©rico e categ√≥rico) em colunas diferentes de forma simult√¢nea.

> üí°**Nota:** Todos os m√©todos foram feitos com o objetivo de funcionar em todos/muitos modelos diferentes de ML, tornando os m√©todos universais e reutiliz√°veis. 


```bash
def ml_preprocess_data(numeric_features: list=[], categorical_features: list = []):
    if numeric_features == []:
            cat_transformer = Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore"))
            ])
            preprocessor = ColumnTransformer(
            transformers=[
            ("cat", cat_transformer, categorical_features)
             ])
    elif categorical_features == []:
            num_transformer = Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="mean")),
            ("minmaxscaler", MinMaxScaler())
            ])
            preprocessor = ColumnTransformer(
            transformers=[
            ("num", num_transformer, numeric_features)
             ])
    elif numeric_features == [] and categorical_features == []:
        preprocessor = None
    else:
        num_transformer = Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="mean")),
            ("minmaxscaler", MinMaxScaler())
        ])

        cat_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore"))
        ])


        preprocessor = ColumnTransformer(
        transformers=[
            ("num", num_transformer, numeric_features),
            ("cat", cat_transformer, categorical_features)
        ])
    
    return preprocessor

```

## ü™û Modelos 

<details>
<summary> üå≤ Decision Tree </summary>

| Par√¢metro | Descri√ß√£o |
|:-------|:-----------|
| **`'tree__max_depth': [3, 5, 10, 15, 20, 30, None]`** | Define a profundidade m√°xima da √°rvore, ou seja, quantos n√≠veis de divis√£o podem existir, a faixa `3 a 30` cobre desde √°rvores rasas (gerais, com pouca complexidade) at√© √°rvores profundas (mais ajustadas aos dados). Al√©m disso, `None` foi inclu√≠do para permitir que a √°rvore cres√ßa sem limite de profundidade, testando o caso extremo. |
| **`'tree__min_samples_split': [3, 5, 10, 15, 20, 30]`** | Indica o m√≠nimo de amostras necess√°rias para dividir um n√≥, valores de 3 a 30 seguem a mesma escala para garantir consist√™ncia, al√©m disso, quanto maior o valor, mais amostras s√£o exigidas para criar novos n√≥s, o que reduz overfitting. |
| **`tree__min_samples_leaf': [3, 5, 10, 15, 20, 30]`** | Define o m√≠nimo de amostras que cada n√≥ folha deve conter, segue a mesma faixa de valores de 3 a 30 para manter coer√™ncia na escala de controle de complexidade. para manter coer√™ncia na escala de controle de complexidade. |
| **`'tree__criterion': ['gini', 'entropy', 'log_loss']`** | Foram usados os 3 crit√©rios dispon√≠veis segundo a documenta√ß√£o do **`scikit-learn`**. `gini` = padr√£o, eficiente e simples; `entropy` = considera a impureza de forma mais detalhada; `log_loss` = mais sens√≠vel a probabilidades previstas. |
| **`'tree__min_weight_fraction_leaf': [0.0, 0.1, 0.15, 0.2, 0.3, 0.5]`** | Determina a fra√ß√£o m√≠nima do peso total das amostras necess√°ria em cada folha, testa de 0 (sem restri√ß√£o) at√© 0.5 (folhas muito grandes), cobrindo escalas pequenas e m√©dias.  |
| **`'tree__max_features': [None]`** | Indica quantas features s√£o consideradas para cada divis√£o, mantido como None para usar todas as vari√°veis dispon√≠veis.  |
| **`'tree__random_state': [42]`** | Define a semente aleat√≥ria para reprodutibilidade, o valor fixo 42 √© padr√£o e facilita reproduzir resultados. |
| **`'tree__max_leaf_nodes': [None, 10, 20, 30, 50]`** | Limita o n√∫mero m√°ximo de folhas, valores 10 a 50 seguem a escala crescente (N√∫meros maiores considerando que s√£o 10k de dados) e `None` para dar liberdade de crescimento livre. |

```bash
import pandas as pd
def decision_tree(data: pd.DataFrame):
    from sklearn.pipeline import Pipeline
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.model_selection import train_test_split
    from sklearn.model_selection import GridSearchCV
    from sklearn.preprocessing import LabelEncoder
    from sklearn.metrics import classification_report
    from sklearn.metrics import accuracy_score
    import functions as fn

    # === Carregar vari√°veis de ambiente ===

    X, y = fn.ml_separate_features_and_target(data, 'classificacao_emissao')
    y_encoded = LabelEncoder().fit_transform(y)
    df_num_columns = fn.ml_get_data_numeric(X)
    df_cat_columns = fn.ml_get_data_string(X, 'classificacao_emissao')
    preprocessor = fn.ml_preprocess_data(df_num_columns, df_cat_columns)
    model = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('tree', DecisionTreeClassifier(max_depth=3, random_state=0))
    ])
    # dividindo em conjunto de treino e test
    X_train, X_test, y_train, y_test = train_test_split(X,y_encoded,
                                                        test_size=0.2, 
                                                        random_state=42)
        
    param_grid = {
        'tree__max_depth': [3, 5, 10, 15, 20, 30, None],       # Profundidades - Fazer range em tds
        # removed None from min_samples_split to avoid invalid type errors
        'tree__min_samples_split': [3, 5, 10, 15, 20, 30],     # Min. para dividir n√≥
        'tree__min_samples_leaf': [3, 5, 10, 15, 20, 30],      # Min. em folha
        'tree__criterion': ['gini', 'entropy', 'log_loss'],    # Crit√©rio de divis√£o
        'tree__min_weight_fraction_leaf': [0.0, 0.1, 0.15, 0.2, 0.3, 0.5], # Fra√ß√£o m√≠nima de peso na folha
        'tree__max_features': [None],                          # N√∫mero m√°ximo de features consideradas para divis√£o
        'tree__random_state': [42],                            # Semente para reprodutibilidade
        'tree__max_leaf_nodes': [None, 10, 20, 30, 50],        # N√∫mero m√°ximo de n√≥s folha
        #min_impurity_decrease, class_weight e ccp_alpha n√£o foram adicionados visando o objetivo de que a √°rvore tenha a liberdade de se aprofundar e se ajustar aos dados existentes, uma vez que eles ser√£o inseridos em um banco de dados real e din√¢mico, onde novos dados ser√£o constantemente adicionados.
    }


    grid = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        cv=5, #Isso aq √© cross validation, 5 para balancear entre tempo e performance
        scoring='accuracy'
    )

    grid.fit(X_train, y_train)
    #Usando pipeline, ele j√° tratou todos os dados usados adiante
    best_model = grid.best_estimator_
    best_model.fit(X_train, y_train)
    y_pred = best_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    return best_model, classification_report(y_test, y_pred, output_dict=True), accuracy


```

</details>

<details>
<summary> üü¢ KNN </summary>

| Par√¢metro | Descri√ß√£o |
|:-------|:-----------|
| **`'classifier__n_neighbors': [3, 5, 7, 9]`** | N√∫mero de vizinhos considerados para classificar uma amostra, valores √≠mpares de 3 a 9 foram escolhidos para evitar empates e manter a coer√™ncia com a escala usada faixa 3 a 30. Quanto maior o n√∫mero, mais ‚Äúsuave‚Äù e generalizado o modelo e quanto menor, mais sens√≠vel aos ru√≠dos. |
| **`'classifier__weights': ['uniform', 'distance']`** | Define como cada vizinho contribui na decis√£o. `uniform`: todos os vizinhos t√™m peso igual; `distance`: vizinhos mais pr√≥ximos t√™m mais peso.  |
| **`'classifier__metric': ['euclidean', 'manhattan','minkowski']`** | Define a m√©trica de dist√¢ncia usada para calcular a proximidade entre pontos. `euclidean`: dist√¢ncia padr√£o (reta entre dois pontos); `manhattan`: soma das dist√¢ncias absolutas, √∫til em dados com muitas features independentes; `minkowski`: generaliza o `euclidean` e `manhattan` (controlada pelo par√¢metro p). |
| **`'classifier__p': [1, 2, 3]`** | Define o expoente da dist√¢ncia de Minkowski. `p=1` = Manhattan; `p=2` = Euclidiana, `p=3` = dist√¢ncia c√∫bica (mais sens√≠vel a grandes diferen√ßas). |
| **`'classifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']`** | Define o algoritmo usado para encontrar os vizinhos mais pr√≥ximos. `auto`: o sklearn escolhe automaticamente o m√©todo mais eficiente; `ball_tree` e `kd_tree`: m√©todos baseados em estruturas de √°rvore; `brute`: faz busca direta (mais lento, mas garante exatid√£o).   |
| **`'classifier__n_jobs': [-1]`** | Define quantos n√∫cleos do processador usar, -1 = usa todos os n√∫cleos dispon√≠veis.  |


```bash
import pandas as pd
def knn(data: pd.DataFrame):
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.model_selection import GridSearchCV
    from sklearn.pipeline import Pipeline
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report
    from sklearn.metrics import accuracy_score
    from sklearn.preprocessing import LabelEncoder
    import functions as fn

    X, y = fn.ml_separate_features_and_target(data, 'classificacao_emissao')
    y_encoded = LabelEncoder().fit_transform(y)
    df_num_columns = fn.ml_get_data_numeric(X)
    df_cat_columns = fn.ml_get_data_string(X, 'classificacao_emissao')
    preprocessor = fn.ml_preprocess_data(df_num_columns, df_cat_columns)
    model = Pipeline(steps=[
        ("preprocessor", preprocessor),
        ("classifier", KNeighborsClassifier())
    ])
    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    param_grid = {
        'classifier__n_neighbors': [3, 5, 7, 9],
        'classifier__weights': ['uniform', 'distance'],
        'classifier__metric': ['euclidean', 'manhattan','minkowski'], #botei todas as m√©tricas pra ele testar de td
        'classifier__p': [1, 2, 3], #tenho q ver se isso aq vai coisar o modelo
        'classifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
        'classifier__leaf_size': [20, 30, 40, 50], #O padr√£o para 10k √© 30, mas com buscamos o melhor modelo, visei testar outros valores, seguindo o padr√£o de 5 elementos usado na DecisionTree
        'classifier__n_jobs': [-1] #Usar todos os n√∫cleos dispon√≠veis -> S√£o 10k de dados
    }

    grid = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')
    grid.fit(X_train, y_train)
    best_model = grid.best_estimator_
    best_model.fit(X_train, y_train)
    y_pred = best_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    return best_model, classification_report(y_test, y_pred, output_dict=True), accuracy


```
</details>

<details>
<summary> üìà LogisticRegressionCV </summary>

| Par√¢metro | Descri√ß√£o |
|:-------|:-----------|
| **`Cs=[3,5, 10, 20,30]`** | Define os valores do hiperpar√¢metro de regulariza√ß√£o C (inverso da for√ßa de regulariza√ß√£o), q2uanto menor o n√∫mero, maior √© a regulariza√ß√£o. |
| **`max_iter=6000`** | Define o n√∫mero m√°ximo de itera√ß√µes para o algoritmo convergir, o solver saga √© mais custoso e, com elasticnet, exige mais itera√ß√µes e 6000 garante converg√™ncia mesmo em bases grandes (como 10k linhas). |
| **`solver='saga'`** | Define o algoritmo usado para otimiza√ß√£o, `saga` √© o √∫nico solver que suporta penalty='elasticnet' e multi_class='multinomial', √© escal√°vel e eficiente para grandes datasets. |
| **`penalty='elasticnet'`** | Define o tipo de regulariza√ß√£o aplicada. O elasticnet combina `L1` e `L2`. `L1` =  zera alguns coeficientes, sele√ß√£o de features; `L2` =  encolhe coeficientes suavemente, estabilidade. |
| **`class_weight='balanced'`** | Compensa desbalanceamentos de classes, isso evita que classes majorit√°rias dominem o modelo, melhorando o recall das classes minorit√°rias. |
| **`cv=10`** | N√∫mero de divis√µes na valida√ß√£o cruzada interna, o valor 10 √© um padr√£o robusto, oferecendo boa estimativa de generaliza√ß√£o sem exagerar no tempo de treino. Isso reduz a vari√¢ncia nas m√©tricas de valida√ß√£o e melhora a confian√ßa nos hiperpar√¢metros escolhidos. |
| **`random_state=42`** | Semente fixa para reprodutibilidade, garante que os mesmos resultados possam ser reproduzidos em execu√ß√µes futuras. |
| **`n_jobs=-1`** | Utiliza todos os n√∫cleos de CPU dispon√≠veis, isso √© essencial para acelerar o LogisticRegressionCV, que realiza m√∫ltiplos treinos paralelamente. Al√©m disso, otimiza o tempo de execu√ß√£o, especialmente com cv=10 e Cs m√∫ltiplos. |
| **`verbose=1`** | Exibe o progresso do treinamento durante a execu√ß√£o, √∫til para monitorar o tempo de converg√™ncia e desempenho durante o ajuste com grandes bases. |
| **`multi_class='multinomial'`** | Define a estrat√©gia para problemas multiclasse, `multinomial` treina todas as classes simultaneamente, ao contr√°rio de `ovr` (one-vs-rest). Isso fornece previs√µes mais consistentes quando h√° m√∫ltiplas classes e interdepend√™ncia entre elas, ideal para datasets com v√°rias categorias de emiss√£o. |
| **`l1_ratios=[0.1, 0.5, 0.9]`** | Define a propor√ß√£o entre L1 e L2 na penaliza√ß√£o elasticnet, permite testar diferentes graus de regulariza√ß√£o combinada, ajustando o modelo √† complexidade dos dados. Quanto maior o n√∫mero, mais agressiva √© a regulariza√ß√£o na sele√ß√£o de vari√°veis. |


> üí° **Nota:** o LogisticRegressionCV √© o LogisticRegression mais otimizado, como o C cr√≠tico para mostrar a regulariza√ß√£o √© importante, o scikit-learn tamb√©m disponibiliza uma vers√£o j√° com Cross Validation do Logistic Regression. Resumindo, √© o Logistic Regression com GridSearchCV implementado.


```bash
import pandas as pd
def LogisticRegressionCV(data:pd.DataFrame):
    from sklearn.Logistic_model import LogisticRegressionCV
    from sklearn.pipeline import Pipeline
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report
    from sklearn.preprocessing import LabelEncoder
    from sklearn.metrics import accuracy_score
    import functions as fn

    X, y = fn.ml_separate_features_and_target(data, 'classificacao_emissao')
    y_encoded = LabelEncoder().fit_transform(y)
    df_num_columns = fn.ml_get_data_numeric(X)
    df_cat_columns = fn.ml_get_data_string(X, 'classificacao_emissao')
    preprocessor = fn.ml_preprocess_data(df_num_columns, df_cat_columns)
    if preprocessor is not None:
        model = Pipeline(steps=[
            ("preprocessor", preprocessor),
            ("classifier", LogisticRegressionCV(
                Cs=[3,5, 10, 20,30],
                max_iter=6000, #como eu coloquei saga e elasticnet, o ideal √© aumentar o max_iter - MAS AUMENTAR MUITO MESMO -> eles demoram pacas
                solver='saga', # 'saga' suporta penalty='elasticnet' e multi_class='multinomial'
                penalty='elasticnet', #vou deixar assim por enquanto pq vou mudar do dataset, o elasticnet combina l1 e l2 - l1 encolhe os coeficientes de forma suave e o l2 for√ßa alguns coeficientes a zero; a diferen√ßa est√° no calculo feito e o elasticnet tenta balancear os dois
                class_weight='balanced', #defini o peso balanceado conforme a distribuicao das classes
                cv=10, #deixei 10 para mais robustez
                random_state=42, #Num padr√£o para reprodutibilidade - n√£o influencia mt no resultado
                n_jobs=-1, # Usar todos os n√∫cleos dispon√≠veis para acelerar o treinamento
                verbose=1, # Para ver o progresso do treinamento
                multi_class='multinomial', #Ele treina todas as classes de uma vez ent√£o para 10k de dados √© melhor do que os outros
                l1_ratios=[0.1, 0.5, 0.9] #Mistura l1 e l2 na regulariza√ß√£o
            ))
        ])
    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    return model, classification_report(y_test, y_pred, output_dict=True), accuracy


```

</details>

 > üí° Nota: Para abrir a explica√ß√£o, clique na seta na esquerda. Ela cont√©m o funcionamente e explica√ß√£o final de cada ponto do c√≥digo e tamb√©m da sa√≠da.
---

## Fun√ß√µes utilizadas pelos modelos

| Fun√ß√£o | Descri√ß√£o |
|:-------|:-----------|
| **`ml_separate_features_and_target(data: pd.DataFrame, target_column: str)`** | Separa o target das demais colunas. |
| **`ml_get_data_string(data: pd.DataFrame, target_column: str)`** | Retorna uma lista com os nomes das colunas categ√≥ricas (tipo texto ou categoria), excluindo a coluna-alvo. |
| **`ml_get_data_numeric(data: pd.DataFrame)`** | Retorna uma lista com os nomes das colunas num√©ricas do DataFrame. |
| **`ml_preprocess_data(numeric_features: list=[], categorical_features: list = [])`** | Cria o pr√©-processador de dados (pipeline do scikit-learn) com base no tipo de vari√°vel. *(Explica√ß√£o do pr√©-processamento acima)* |
| **`dt_get_full_conection()`** | faz a conex√£o do `PostgresSQL` e no `MongoDB` |
| **`dt_get_data()`** | Junta dados das duas fontes, `MongoDB` e `PostgreSQL` |
| **`save_model(model, filename: str, folder: str = "best_model")`** | Salva o modelo treinado em um arquivo .joblib |

>üí° **Nota:** Fun√ß√µes com o prefixo `ml` s√£o fun√ß√µes para os modelos e fun√ß√µes com o prefixo `dt` s√£o fun√ß√µes para tratamento e processamento de dados.



---

## ü§ñ Passo a Passo de como funciona o orquestrador - `choose_a_model.py`

- Importar bibliotecas necess√°rias:
  
| Import | Descri√ß√£o |
|:-------|:-----------|
| **`collections`** | defaultdict √© usado para criar um dicion√°rio que inicializa automaticamente valores padr√£o (no caso, inteiros iniciando em 0), facilita a contagem de "vit√≥rias" dos modelos, sem precisar checar se a chave existe.  |
| **`Modelos`** | Importa√ß√£o de cada arquivo `.py` contendo os modelos criados pelo time Calbon |
| **`functions`** | Fun√ß√µes de `functions.py`, *(especificadas acima)* |

```bash

from collections import defaultdict
from decision_tree import decision_tree
from LogisticRegressionCV import LogisticRegressionCV
import functions as fn
from knn import knn

```

##

- Carregamento dos dados
  
```bash
data = fn.dt_get_data()
```


##
- Treinamento dos modelos e pegar as m√©tricas, a acur√°cia tamb√©m vem para casos de desempate 
  
```bash
tree_model, tree_report, tree_accuracy = decision_tree(data)
logreg_model, logreg_report, logreg_accuracy = logisticRegressionCV(data)
knn_model, knn_report, knn_accuracy = knn(data)

```

##
- Organiza os resultados

```bash
results = {
    "Decision Tree": {"model": tree_model, "report": tree_report, "accuracy": tree_accuracy},
    "Logistic Regression CV": {"model": logreg_model, "report": logreg_report, "accuracy": logreg_accuracy},
    "KNN": {"model": knn_model, "report": knn_report, "accuracy": knn_accuracy}
}

```
##
- Prepara√ß√£o das m√©tricas, aqui define quais m√©tricas ser√£o usadas na compara√ß√£o entre os modelos.
```bash
metric_fields = ["precision", "recall", "f1-score"]
```
##
- Junta todas as classes (ou r√≥tulos) existentes nos relat√≥rios do `report`
```bash
metric_fields = ["precision", "recall", "f1-score"]
```
##
- Armazena quantas vezes cada modelo teve a melhor m√©trica.
```bash
  wins = defaultdict(int)
```
##
- Compara cada modelo em cada m√©trica (precision, recall e f1-score) para cada classe.
```bash
all_keys = set()
for data in results.values():
    rpt = data["report"]
    if isinstance(rpt, str):
        raise RuntimeError("classification reports must be dicts. Use output_dict=True when calling classification_report.")
    all_keys.update(rpt.keys())

  for key in all_keys:
    for field in metric_fields:
        values = {}
        for name, data in results.items():
            rpt = data["report"]
            try:
                val = rpt[key][field]
            except Exception:
                continue
            try:
                values[name] = float(val)
            except Exception:
                continue
        if not values:
            continue

        max_val = max(values.values())
        for name, v in values.items():
            if v == max_val:
                wins[name] += 1
```
##
- Compara√ß√£o por acur√°cia
```bash
acc_values = {}
for name, data in results.items():
    try:
        acc_values[name] = float(data["accuracy"])
    except Exception:
        continue

if acc_values:
    rounded = {n: round(v, 6) for n, v in acc_values.items()}
    max_acc = max(rounded.values())
    for n, v in rounded.items():
        if v == max_acc:
            wins[n] += 1
```
##
- Escolhe o melhor modelo, printa o resumo final e salva
```bash
best_name = max(results.keys(), key=lambda n: (wins.get(n, 0), results[n].get("accuracy", 0)))
best_model = results[best_name]["model"]

print("wins per model:", dict(wins))
print("chosen model:", best_name)

for name, data in results.items():
    if name == best_name:
        fn.save_model(data["model"], "best_model.pkl")
```

---

<h3 align="center">‚ú® Desenvolvido para CALBON - Treinamento de Modelo de Predi√ß√£o üåø</h3>
